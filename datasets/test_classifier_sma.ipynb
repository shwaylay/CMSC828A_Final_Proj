{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data.sampler import SubsetRandomSampler, SequentialSampler\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import glob\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import torch\n",
    "from glob import glob\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, folder_nolayoff, folder_layoff, src_col):\n",
    "        \n",
    "        # csvs_layoffs = glob.glob(folder_layoff + \"/*.csv\")\n",
    "        # csvs_nolayoffs = glob.glob(folder_nolayoff + \"/*.csv\")\n",
    "        X_train = []\n",
    "        Y_train = []\n",
    "        all_csv_files = []\n",
    "        EXT = \"*.csv\"  # Define the variable EXT\n",
    "        for path, subdir, files in os.walk(folder_layoff):\n",
    "            \n",
    "            for file in glob(os.path.join(path, EXT)):\n",
    "                # print(file)\n",
    "                df = pd.read_csv(file, index_col=0, parse_dates=True).sort_index()\n",
    "                ## If length of dataset is over 90, then cut it to 90\n",
    "                if len(df) > 90:\n",
    "                    print(\"OVER LENGTH\", file)\n",
    "                    df = df[-90:]\n",
    "                ## If length of dataset is less than 90, then skip it\n",
    "                if len(df) < 90:\n",
    "                    continue\n",
    "                X_train.append(df[src_col].values)\n",
    "                Y_train.append(float(1))\n",
    "                # all_csv_files.append(file)\n",
    "\n",
    "        for path, subdir, files in os.walk(folder_nolayoff):\n",
    "            for file in glob(os.path.join(path, EXT)):\n",
    "                df = pd.read_csv(file, index_col=0, parse_dates=True).sort_index()\n",
    "                ## If length of dataset is over 90, then cut it to 90\n",
    "                if len(df) > 90:\n",
    "                    print(\"OVER LENGTH\", file)\n",
    "                    df = df[-90:]\n",
    "\n",
    "                ## If length of dataset is less than 90, then skip it\n",
    "                if len(df) < 90:\n",
    "                    continue\n",
    "                X_train.append(df[src_col].values)\n",
    "                Y_train.append(float(0))\n",
    "                # all_csv_files.append(file)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # for directory in csvs_layoffs:\n",
    "        #     for csv in os.listdir(directory):\n",
    "        #         if csv.endswith(\".csv\"):\n",
    "        #             df = pd.read_csv(csv, index_col=0, parse_dates=True).sort_index()\n",
    "        #             X_train.append(df[src_col].values)\n",
    "        #             Y_train.append(1)\n",
    "        #     # df = pd.read_csv(csv, index_col=0, parse_dates=True).sort_index()\n",
    "        #     # X_train.append(df[src_col].values)\n",
    "        #     # Y_train.append(1)\n",
    "        # for directory in csvs_nolayoffs:\n",
    "        #     for csv in os.listdir(directory):\n",
    "        #         if csv.endswith(\".csv\"):\n",
    "        #             df = pd.read_csv(csv, index_col=0, parse_dates=True).sort_index()\n",
    "        #             X_train.append(df[src_col].values)\n",
    "        #             Y_train.append(0)\n",
    "            \n",
    "        X_train = torch.from_numpy(np.array(X_train))\n",
    "        Y_train = torch.from_numpy(np.array(Y_train))\n",
    "        # if len(X_train.shape) < 3:\n",
    "        #     X_train = X_train.unsqueeze(2)\n",
    "\n",
    "        # if X_train.shape.index(min(X_train.shape[1], X_train.shape[2])) != 1:  # make sure the Channels in second dim\n",
    "        #     X_train = X_train.permute(0, 2, 1)\n",
    "\n",
    "        self.X_train = X_train\n",
    "        self.Y_train = Y_train\n",
    "        self.num_channels = X_train.shape[1]\n",
    "        self.len = X_train.shape[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X_train[idx].float(), self.Y_train[idx]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"stocks_layoffs/Energy/ENPH0.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avg = list(df['Open'].rolling(window=90).mean())[-1]\n",
    "# df['Normalized_Price_SMA90'] = df['Open'] / avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[\"Normalized_Price_SMA90\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sma90 = TimeSeriesDataset(\"stocks_no_layoffs\", \"stocks_layoffs\", \"Normalized_Price_SMA90\")\n",
    "dataset_oipa = TimeSeriesDataset(\"stocks_no_layoffs\", \"stocks_layoffs\", \"open_inproportion_to_average\")\n",
    "dataset_on = TimeSeriesDataset(\"stocks_no_layoffs\", \"stocks_layoffs\", \"open_normalized\")\n",
    "batch_size = 1\n",
    "validation_split = .2\n",
    "shuffle_dataset = True\n",
    "random_seed= 42\n",
    "\n",
    "# Creating data indices for training and validation splits:\n",
    "dataset_size = len(dataset_sma90)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SequentialSampler(val_indices)\n",
    "\n",
    "train_loader_sma90 = torch.utils.data.DataLoader(dataset_sma90, batch_size=batch_size, \n",
    "                                           sampler=train_sampler)\n",
    "train_loader_oipa = torch.utils.data.DataLoader(dataset_oipa, batch_size=batch_size, \n",
    "                                           sampler=train_sampler)\n",
    "train_loader_on = torch.utils.data.DataLoader(dataset_on, batch_size=batch_size, \n",
    "                                           sampler=train_sampler)\n",
    "\n",
    "validation_loader_sma90 = torch.utils.data.DataLoader(dataset_sma90, batch_size=batch_size, sampler=valid_sampler)\n",
    "validation_loader_oipa = torch.utils.data.DataLoader(dataset_oipa, batch_size=batch_size, sampler=valid_sampler)\n",
    "validation_loader_on = torch.utils.data.DataLoader(dataset_on, batch_size=batch_size,   sampler=valid_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader_sma90.dataset.Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tdloader = TimeSeriesDataset(\"stocks_no_layoffs\", \"stocks_layoffs\", \"open_in_proportion_to_average\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layoff_data_loader = DataLoader(dataset=tdloader, batch_size=1, \n",
    "#                                 shuffle=True, drop_last=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layoff_data_loader.dataset.X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Iterate through dataloader and print y labels\n",
    "# for i, (x, y) in enumerate(validation_loader_sma90):\n",
    "#     print(y)\n",
    "#     # print(x)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Iterate through dataloader and print y labels\n",
    "# for i, (x, y) in enumerate(train_loader_oipa):\n",
    "#     print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader_oipa.dataset.Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alexs\\anaconda3\\envs\\cs828\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss_sma90: 0.6291, Loss_oipa: 0.7264, Loss_on: 0.7136\n",
      "Epoch [2/50], Loss_sma90: 0.7100, Loss_oipa: 0.6798, Loss_on: 0.6949\n",
      "Epoch [3/50], Loss_sma90: 0.6477, Loss_oipa: 0.7826, Loss_on: 1.0638\n",
      "Epoch [4/50], Loss_sma90: 0.6850, Loss_oipa: 0.7472, Loss_on: 0.5080\n",
      "Epoch [5/50], Loss_sma90: 0.6946, Loss_oipa: 0.4010, Loss_on: 0.3976\n",
      "Epoch [6/50], Loss_sma90: 0.7202, Loss_oipa: 0.6258, Loss_on: 0.2832\n",
      "Epoch [7/50], Loss_sma90: 0.6682, Loss_oipa: 0.6652, Loss_on: 0.4722\n",
      "Epoch [8/50], Loss_sma90: 0.6518, Loss_oipa: 0.2769, Loss_on: 0.4101\n",
      "Epoch [9/50], Loss_sma90: 0.6561, Loss_oipa: 0.2015, Loss_on: 0.5378\n",
      "Epoch [10/50], Loss_sma90: 0.6505, Loss_oipa: 0.7299, Loss_on: 0.8502\n",
      "Epoch [11/50], Loss_sma90: 0.6486, Loss_oipa: 0.1625, Loss_on: 0.1232\n",
      "Epoch [12/50], Loss_sma90: 0.7402, Loss_oipa: 0.6607, Loss_on: 0.1553\n",
      "Epoch [13/50], Loss_sma90: 0.7147, Loss_oipa: 0.3114, Loss_on: 0.2934\n",
      "Epoch [14/50], Loss_sma90: 0.7265, Loss_oipa: 0.1385, Loss_on: 0.0444\n",
      "Epoch [15/50], Loss_sma90: 0.7037, Loss_oipa: 1.2737, Loss_on: 0.0117\n",
      "Epoch [16/50], Loss_sma90: 0.7392, Loss_oipa: 0.6176, Loss_on: 0.0967\n",
      "Epoch [17/50], Loss_sma90: 0.6588, Loss_oipa: 0.0860, Loss_on: 0.2547\n",
      "Epoch [18/50], Loss_sma90: 0.5338, Loss_oipa: 0.7858, Loss_on: 0.0617\n",
      "Epoch [19/50], Loss_sma90: 0.7404, Loss_oipa: 1.4326, Loss_on: 1.1886\n",
      "Epoch [20/50], Loss_sma90: 0.7396, Loss_oipa: 0.8824, Loss_on: 0.0064\n",
      "Epoch [21/50], Loss_sma90: 0.6346, Loss_oipa: 0.1352, Loss_on: 0.3126\n",
      "Epoch [22/50], Loss_sma90: 0.7600, Loss_oipa: 0.0022, Loss_on: 0.3192\n",
      "Epoch [23/50], Loss_sma90: 0.6287, Loss_oipa: 0.3316, Loss_on: 0.9234\n",
      "Epoch [24/50], Loss_sma90: 0.6233, Loss_oipa: 0.2286, Loss_on: 0.0523\n",
      "Epoch [25/50], Loss_sma90: 0.1297, Loss_oipa: 0.8931, Loss_on: 0.0064\n",
      "Epoch [26/50], Loss_sma90: 0.6290, Loss_oipa: 0.2836, Loss_on: 1.2709\n",
      "Epoch [27/50], Loss_sma90: 0.6168, Loss_oipa: 0.1375, Loss_on: 0.0206\n",
      "Epoch [28/50], Loss_sma90: 0.6176, Loss_oipa: 0.0082, Loss_on: 0.0363\n",
      "Epoch [29/50], Loss_sma90: 0.7790, Loss_oipa: 0.0062, Loss_on: 1.2325\n",
      "Epoch [30/50], Loss_sma90: 0.4528, Loss_oipa: 0.0887, Loss_on: 0.0001\n",
      "Epoch [31/50], Loss_sma90: 0.7777, Loss_oipa: 0.0343, Loss_on: 0.0131\n",
      "Epoch [32/50], Loss_sma90: 0.7810, Loss_oipa: 0.3209, Loss_on: 0.0002\n",
      "Epoch [33/50], Loss_sma90: 0.7787, Loss_oipa: 0.0628, Loss_on: 0.0515\n",
      "Epoch [34/50], Loss_sma90: 0.6181, Loss_oipa: 0.0145, Loss_on: 0.1042\n",
      "Epoch [35/50], Loss_sma90: 0.6183, Loss_oipa: 0.4456, Loss_on: 0.0009\n",
      "Epoch [36/50], Loss_sma90: 0.2191, Loss_oipa: 0.0335, Loss_on: 5.4298\n",
      "Epoch [37/50], Loss_sma90: 0.4172, Loss_oipa: 0.0231, Loss_on: 0.0014\n",
      "Epoch [38/50], Loss_sma90: 0.6202, Loss_oipa: 0.0222, Loss_on: 0.0000\n",
      "Epoch [39/50], Loss_sma90: 0.6185, Loss_oipa: 0.5293, Loss_on: 0.0042\n",
      "Epoch [40/50], Loss_sma90: 0.7729, Loss_oipa: 0.0263, Loss_on: 0.0002\n",
      "Epoch [41/50], Loss_sma90: 0.6204, Loss_oipa: 0.0002, Loss_on: 0.0001\n",
      "Epoch [42/50], Loss_sma90: 0.6206, Loss_oipa: 0.6211, Loss_on: 0.0004\n",
      "Epoch [43/50], Loss_sma90: 0.6185, Loss_oipa: 0.0517, Loss_on: 0.0035\n",
      "Epoch [44/50], Loss_sma90: 0.6214, Loss_oipa: 0.0206, Loss_on: 0.0005\n",
      "Epoch [45/50], Loss_sma90: 0.7702, Loss_oipa: 0.0642, Loss_on: 0.0209\n",
      "Epoch [46/50], Loss_sma90: 0.7694, Loss_oipa: 0.0602, Loss_on: 0.0001\n",
      "Epoch [47/50], Loss_sma90: 0.0085, Loss_oipa: 0.0325, Loss_on: 0.0067\n",
      "Epoch [48/50], Loss_sma90: 0.7560, Loss_oipa: 0.0024, Loss_on: 0.1208\n",
      "Epoch [49/50], Loss_sma90: 0.6227, Loss_oipa: 0.0000, Loss_on: 0.0006\n",
      "Epoch [50/50], Loss_sma90: 0.7683, Loss_oipa: 0.0019, Loss_on: 0.0056\n"
     ]
    }
   ],
   "source": [
    "# Define the model architecture\n",
    "class BinaryClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BinaryClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(90, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.sigmoid(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "# Create an instance of the model\n",
    "model_sma_90d = BinaryClassifier()\n",
    "model_open_inproportion_to_average = BinaryClassifier()\n",
    "model_open_normalized = BinaryClassifier()\n",
    "# Define the loss function and optimizer\n",
    "criterion_sma90 = nn.BCELoss()\n",
    "criterion_oipa = nn.BCELoss()\n",
    "criterion_on = nn.BCELoss()\n",
    "optimizer_sma90 = optim.Adam(model_sma_90d.parameters())\n",
    "optimizer_oipa = optim.Adam(model_open_inproportion_to_average.parameters())\n",
    "optimizer_on = optim.Adam(model_open_normalized.parameters())\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 50\n",
    "batch_size = 32\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    ## Run through dataloader batches\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    for i, (X_train, y_train) in enumerate(train_loader_sma90):\n",
    "    \n",
    "        # Forward pass\n",
    "        # print(X_train)\n",
    "        outputs_sma_90d = model_sma_90d(X_train)\n",
    "        y_train = y_train.unsqueeze(1)\n",
    "        loss_sma90 = criterion_sma90(outputs_sma_90d.float(), y_train.float())\n",
    "        optimizer_sma90.zero_grad()\n",
    "        loss_sma90.backward()\n",
    "        optimizer_sma90.step()\n",
    "    for i, (X_train, y_train) in enumerate(train_loader_oipa):\n",
    "        # Forward pass\n",
    "        outputs_open_inproportion_to_average = model_open_inproportion_to_average(X_train)\n",
    "        y_train = y_train.unsqueeze(1)\n",
    "        loss_oipa = criterion_oipa(outputs_open_inproportion_to_average.float(), y_train.float())\n",
    "        optimizer_oipa.zero_grad()\n",
    "        loss_oipa.backward()\n",
    "        optimizer_oipa.step()\n",
    "    for i, (X_train, y_train) in enumerate(train_loader_on):\n",
    "        # Forward pass\n",
    "        outputs_open_normalized = model_open_normalized(X_train)\n",
    "        y_train = y_train.unsqueeze(1)\n",
    "        loss_on = criterion_on(outputs_open_normalized.float(), y_train.float())\n",
    "        # Backward pass and optimization\n",
    "        optimizer_on.zero_grad()\n",
    "        loss_on.backward()\n",
    "        optimizer_on.step()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss_sma90: {loss_sma90.item():.4f}, Loss_oipa: {loss_oipa.item():.4f}, Loss_on: {loss_on.item():.4f}')\n",
    "    \n",
    "\n",
    "# # Evaluation\n",
    "# with torch.no_grad():\n",
    "#     outputs = model(X_test)\n",
    "#     predicted = (outputs >= 0.5).float()\n",
    "#     accuracy = (predicted == y_test).float().mean()\n",
    "#     print(f'Test Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# # Make predictions\n",
    "# with torch.no_grad():\n",
    "#     outputs = model(X_new)\n",
    "#     predicted = (outputs >= 0.5).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single model eval\n",
    "# is_correct = []\n",
    "# for i, (X_train, y_train) in enumerate(validation_loader_sma90):\n",
    "    \n",
    "\n",
    "#         # print(X_train)\n",
    "#         # print(y_train)\n",
    "#         # Forward pass\n",
    "#     outputs = model(X_train)\n",
    "#     out = outputs.squeeze(1)\n",
    "#     output_bin = (out >= 0.5).int().item()\n",
    "#     correct = output_bin == int(y_train.item())\n",
    "#     is_correct.append(correct)\n",
    "# # Count ratio of trues and falses in is_correct\n",
    "# true_count = sum(is_correct)\n",
    "# false_count = len(is_correct) - true_count\n",
    "# print(f\"True count: {true_count}\")\n",
    "# print(f\"False count: {false_count}\")\n",
    "# print(f\"True ratio: {true_count / len(is_correct)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True count (Confidence): 91\n",
      "False count (Confidence): 12\n",
      "True ratio (Confidence): 0.883495145631068\n",
      "True count (Vote): 85\n",
      "False count (Vote): 18\n",
      "True ratio (Vote): 0.8252427184466019\n",
      "True count (sma90): 14\n",
      "False count (sma90): 89\n",
      "True ratio (sma90): 0.13592233009708737\n",
      "True count (OIPA): 91\n",
      "False count (OIPA): 12\n",
      "True ratio (OIPA): 0.883495145631068\n",
      "True count (ON): 92\n",
      "False count (ON): 11\n",
      "True ratio (ON): 0.8932038834951457\n"
     ]
    }
   ],
   "source": [
    "# Majority Voting Ensemble Eval\n",
    "is_correct_confidence = []\n",
    "is_correct_vote = []\n",
    "is_correct_sma90 = []\n",
    "is_correct_oipa = []\n",
    "is_correct_on = []\n",
    "\n",
    "for i, (X_train, y_train) in enumerate(validation_loader_sma90):\n",
    "        # print(X_train)\n",
    "        # print(y_train)\n",
    "        # Forward pass\n",
    "    X_train_sma90 = validation_loader_sma90.dataset.X_train[i].unsqueeze(0).float()\n",
    "    X_train_oipa = validation_loader_oipa.dataset.X_train[i].unsqueeze(0).float()\n",
    "    X_train_on = validation_loader_on.dataset.X_train[i].unsqueeze(0).float()\n",
    "    Y_train_sma90 = validation_loader_sma90.dataset.Y_train[i]\n",
    "    Y_train_oipa = validation_loader_oipa.dataset.Y_train[i]\n",
    "    Y_train_on = validation_loader_on.dataset.Y_train[i]\n",
    "    # Sanity check\n",
    "    assert (Y_train_sma90.item() == Y_train_oipa.item())\n",
    "    assert (Y_train_sma90.item() == Y_train_on.item())\n",
    "\n",
    "    \n",
    "    # print(outputs_sma90)\n",
    "    outputs_sma90 = model_sma_90d(X_train_sma90).squeeze(1)\n",
    "    outputs_oipa = model_open_inproportion_to_average(X_train_oipa).squeeze(1)\n",
    "    outputs_on = model_open_normalized(X_train_on).squeeze(1)\n",
    "\n",
    "    ## Ensemble confidence based on average of outputs\n",
    "    confidence_avg_out = (outputs_sma90 + outputs_oipa + outputs_on) / 3\n",
    "    confidence_output_bin = (confidence_avg_out >= 0.5).int().item()\n",
    "\n",
    "    vote_sma90 = (outputs_sma90 >= 0.5).int().item()\n",
    "    is_correct_sma90.append(vote_sma90 == int(y_train.item()))\n",
    "    vote_oipa = (outputs_oipa >= 0.5).int().item()\n",
    "    is_correct_oipa.append(vote_oipa == int(y_train.item()))\n",
    "    vote_on = (outputs_on >= 0.5).int().item()\n",
    "    is_correct_on.append(vote_on == int(y_train.item()))\n",
    "    ## Ensemble voting based on majority voting\n",
    "    vote = (vote_sma90 + vote_oipa + vote_on) >= 2\n",
    "    correct_confidence = confidence_output_bin == int(y_train.item())\n",
    "    correct_vote = vote == int(y_train.item())\n",
    "    is_correct_confidence.append(correct_confidence)\n",
    "    is_correct_vote.append(correct_vote)\n",
    "\n",
    "    # out = outputs.squeeze(1)\n",
    "    # output_bin = (out >= 0.5).int().item()\n",
    "    # correct = output_bin == int(y_train.item())\n",
    "    # is_correct.append(correct)\n",
    "# Count ratio of trues and falses in is_correct\n",
    "true_count_confidence = sum(is_correct_confidence)\n",
    "false_count_confidence = len(is_correct_confidence) - true_count_confidence\n",
    "print(f\"True count (Confidence): {true_count_confidence}\")\n",
    "print(f\"False count (Confidence): {false_count_confidence}\")\n",
    "print(f\"True ratio (Confidence): {true_count_confidence / len(is_correct_confidence)}\")\n",
    "true_count_vote = sum(is_correct_vote)\n",
    "false_count_vote = len(is_correct_vote) - true_count_vote\n",
    "print(f\"True count (Vote): {true_count_vote}\")\n",
    "print(f\"False count (Vote): {false_count_vote}\")\n",
    "print(f\"True ratio (Vote): {true_count_vote / len(is_correct_vote)}\")\n",
    "true_count_sma90 = sum(is_correct_sma90)\n",
    "false_count_sma90 = len(is_correct_sma90) - true_count_sma90\n",
    "print(f\"True count (sma90): {true_count_sma90}\")\n",
    "print(f\"False count (sma90): {false_count_sma90}\")\n",
    "print(f\"True ratio (sma90): {true_count_sma90 / len(is_correct_sma90)}\")\n",
    "true_count_oipa = sum(is_correct_oipa)\n",
    "false_count_oipa = len(is_correct_oipa) - true_count_oipa\n",
    "print(f\"True count (OIPA): {true_count_oipa}\")\n",
    "print(f\"False count (OIPA): {false_count_oipa}\")\n",
    "print(f\"True ratio (OIPA): {true_count_oipa / len(is_correct_oipa)}\")\n",
    "true_count_on = sum(is_correct_on)\n",
    "false_count_on = len(is_correct_on) - true_count_on\n",
    "print(f\"True count (ON): {true_count_on}\")\n",
    "print(f\"False count (ON): {false_count_on}\")\n",
    "print(f\"True ratio (ON): {true_count_on / len(is_correct_on)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Count ratio of trues and falses in is_correct\n",
    "# true_count = sum(is_correct)\n",
    "# false_count = len(is_correct) - true_count\n",
    "# print(f\"True count: {true_count}\")\n",
    "# print(f\"False count: {false_count}\")\n",
    "# print(f\"True ratio: {true_count / len(is_correct)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save each model\n",
    "directory = \"classifier_models/\"\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "torch.save(model_sma_90d.state_dict(), directory +\"sma_90d_model.pth\")\n",
    "torch.save(model_open_inproportion_to_average.state_dict(), directory +\"open_inproportion_to_average_model.pth\")\n",
    "torch.save(model_open_normalized.state_dict(), directory +\"open_normalized_model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in saved models\n",
    "model_sma_90d = BinaryClassifier()\n",
    "model_open_inproportion_to_average = BinaryClassifier()\n",
    "model_open_normalized = BinaryClassifier()\n",
    "model_sma_90d.load_state_dict(torch.load(directory +\"sma_90d_model.pth\"))\n",
    "model_open_inproportion_to_average.load_state_dict(torch.load(directory +\"open_inproportion_to_average_model.pth\"))\n",
    "model_open_normalized.load_state_dict(torch.load(directory +\"open_normalized_model.pth\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs828",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
