{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data.sampler import SubsetRandomSampler, SequentialSampler\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import glob\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import torch\n",
    "from glob import glob\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, folder_nolayoff, folder_layoff, src_col):\n",
    "        \n",
    "        # csvs_layoffs = glob.glob(folder_layoff + \"/*.csv\")\n",
    "        # csvs_nolayoffs = glob.glob(folder_nolayoff + \"/*.csv\")\n",
    "        X_train = []\n",
    "        Y_train = []\n",
    "        all_csv_files = []\n",
    "        EXT = \"*.csv\"  # Define the variable EXT\n",
    "        for path, subdir, files in os.walk(folder_layoff):\n",
    "            \n",
    "            for file in glob(os.path.join(path, EXT)):\n",
    "                # print(file)\n",
    "                df = pd.read_csv(file, index_col=0, parse_dates=True).sort_index()\n",
    "                ## If length of dataset is over 90, then cut it to 90\n",
    "                if len(df) > 90:\n",
    "                    print(\"OVER LENGTH\", file)\n",
    "                    df = df[-90:]\n",
    "                ## If length of dataset is less than 90, then skip it\n",
    "                if len(df) < 90:\n",
    "                    continue\n",
    "                X_train.append(df[src_col].values)\n",
    "                Y_train.append(float(1))\n",
    "                # all_csv_files.append(file)\n",
    "\n",
    "        for path, subdir, files in os.walk(folder_nolayoff):\n",
    "            for file in glob(os.path.join(path, EXT)):\n",
    "                df = pd.read_csv(file, index_col=0, parse_dates=True).sort_index()\n",
    "                ## If length of dataset is over 90, then cut it to 90\n",
    "                if len(df) > 90:\n",
    "                    print(\"OVER LENGTH\", file)\n",
    "                    df = df[-90:]\n",
    "\n",
    "                ## If length of dataset is less than 90, then skip it\n",
    "                if len(df) < 90:\n",
    "                    continue\n",
    "                X_train.append(df[src_col].values)\n",
    "                Y_train.append(float(0))\n",
    "                # all_csv_files.append(file)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # for directory in csvs_layoffs:\n",
    "        #     for csv in os.listdir(directory):\n",
    "        #         if csv.endswith(\".csv\"):\n",
    "        #             df = pd.read_csv(csv, index_col=0, parse_dates=True).sort_index()\n",
    "        #             X_train.append(df[src_col].values)\n",
    "        #             Y_train.append(1)\n",
    "        #     # df = pd.read_csv(csv, index_col=0, parse_dates=True).sort_index()\n",
    "        #     # X_train.append(df[src_col].values)\n",
    "        #     # Y_train.append(1)\n",
    "        # for directory in csvs_nolayoffs:\n",
    "        #     for csv in os.listdir(directory):\n",
    "        #         if csv.endswith(\".csv\"):\n",
    "        #             df = pd.read_csv(csv, index_col=0, parse_dates=True).sort_index()\n",
    "        #             X_train.append(df[src_col].values)\n",
    "        #             Y_train.append(0)\n",
    "            \n",
    "        X_train = torch.from_numpy(np.array(X_train))\n",
    "        Y_train = torch.from_numpy(np.array(Y_train))\n",
    "        # if len(X_train.shape) < 3:\n",
    "        #     X_train = X_train.unsqueeze(2)\n",
    "\n",
    "        # if X_train.shape.index(min(X_train.shape[1], X_train.shape[2])) != 1:  # make sure the Channels in second dim\n",
    "        #     X_train = X_train.permute(0, 2, 1)\n",
    "\n",
    "        self.X_train = X_train\n",
    "        self.Y_train = Y_train\n",
    "        self.num_channels = X_train.shape[1]\n",
    "        self.len = X_train.shape[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X_train[idx].float(), self.Y_train[idx]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"stocks_layoffs/Energy/ENPH0.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling_min = list(df['Open'].rolling(window=90).min())[-1]\n",
    "rolling_max = list(df['Open'].rolling(window=90).max())[-1]\n",
    "\n",
    "# Calculate the scaled prices\n",
    "df['Scaled_Price_MinMax'] = (df[\"Open\"] - rolling_min) / (rolling_max - rolling_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avg = list(df['Open'].rolling(window=90).mean())[-1]\n",
    "# df['Normalized_Price_minmax'] = df['Open'] / avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[\"Normalized_Price_minmax\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_minmax = TimeSeriesDataset(\"stocks_no_layoffs\", \"stocks_layoffs\", \"Scaled_Price_MinMax\")\n",
    "dataset_oipa = TimeSeriesDataset(\"stocks_no_layoffs\", \"stocks_layoffs\", \"open_inproportion_to_average\")\n",
    "dataset_on = TimeSeriesDataset(\"stocks_no_layoffs\", \"stocks_layoffs\", \"open_normalized\")\n",
    "batch_size = 1\n",
    "validation_split = .2\n",
    "shuffle_dataset = True\n",
    "random_seed= 42\n",
    "\n",
    "# Creating data indices for training and validation splits:\n",
    "dataset_size = len(dataset_minmax)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SequentialSampler(val_indices)\n",
    "\n",
    "train_loader_minmax = torch.utils.data.DataLoader(dataset_minmax, batch_size=batch_size, \n",
    "                                           sampler=train_sampler)\n",
    "train_loader_oipa = torch.utils.data.DataLoader(dataset_oipa, batch_size=batch_size, \n",
    "                                           sampler=train_sampler)\n",
    "train_loader_on = torch.utils.data.DataLoader(dataset_on, batch_size=batch_size, \n",
    "                                           sampler=train_sampler)\n",
    "\n",
    "validation_loader_minmax = torch.utils.data.DataLoader(dataset_minmax, batch_size=batch_size, sampler=valid_sampler)\n",
    "validation_loader_oipa = torch.utils.data.DataLoader(dataset_oipa, batch_size=batch_size, sampler=valid_sampler)\n",
    "validation_loader_on = torch.utils.data.DataLoader(dataset_on, batch_size=batch_size,   sampler=valid_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader_minmax.dataset.Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alexs\\anaconda3\\envs\\cs828\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss_minmax: 0.6517, Loss_oipa: 0.6838, Loss_on: 0.8064\n",
      "Epoch [2/50], Loss_minmax: 0.7514, Loss_oipa: 0.7585, Loss_on: 0.3687\n",
      "Epoch [3/50], Loss_minmax: 0.7283, Loss_oipa: 0.5672, Loss_on: 0.9868\n",
      "Epoch [4/50], Loss_minmax: 0.6424, Loss_oipa: 0.7276, Loss_on: 0.5601\n",
      "Epoch [5/50], Loss_minmax: 0.8659, Loss_oipa: 0.5075, Loss_on: 0.7166\n",
      "Epoch [6/50], Loss_minmax: 0.5788, Loss_oipa: 0.2813, Loss_on: 0.6219\n",
      "Epoch [7/50], Loss_minmax: 0.6663, Loss_oipa: 0.2739, Loss_on: 0.7479\n",
      "Epoch [8/50], Loss_minmax: 0.6101, Loss_oipa: 0.8635, Loss_on: 0.3153\n",
      "Epoch [9/50], Loss_minmax: 0.8128, Loss_oipa: 0.2537, Loss_on: 0.6387\n",
      "Epoch [10/50], Loss_minmax: 0.6501, Loss_oipa: 0.3386, Loss_on: 0.1208\n",
      "Epoch [11/50], Loss_minmax: 0.5572, Loss_oipa: 0.7888, Loss_on: 0.6261\n",
      "Epoch [12/50], Loss_minmax: 0.3159, Loss_oipa: 0.5283, Loss_on: 0.2896\n",
      "Epoch [13/50], Loss_minmax: 0.8448, Loss_oipa: 0.8747, Loss_on: 0.2700\n",
      "Epoch [14/50], Loss_minmax: 0.9982, Loss_oipa: 0.6149, Loss_on: 0.2547\n",
      "Epoch [15/50], Loss_minmax: 0.9389, Loss_oipa: 0.7473, Loss_on: 0.4905\n",
      "Epoch [16/50], Loss_minmax: 0.4814, Loss_oipa: 0.7426, Loss_on: 2.3560\n",
      "Epoch [17/50], Loss_minmax: 0.3178, Loss_oipa: 1.6484, Loss_on: 0.1391\n",
      "Epoch [18/50], Loss_minmax: 0.4886, Loss_oipa: 0.1568, Loss_on: 0.0226\n",
      "Epoch [19/50], Loss_minmax: 0.7917, Loss_oipa: 0.1646, Loss_on: 0.1827\n",
      "Epoch [20/50], Loss_minmax: 0.2744, Loss_oipa: 0.2206, Loss_on: 0.0104\n",
      "Epoch [21/50], Loss_minmax: 1.1076, Loss_oipa: 0.1984, Loss_on: 0.0935\n",
      "Epoch [22/50], Loss_minmax: 0.4067, Loss_oipa: 0.2050, Loss_on: 0.0011\n",
      "Epoch [23/50], Loss_minmax: 0.4786, Loss_oipa: 2.3058, Loss_on: 0.7823\n",
      "Epoch [24/50], Loss_minmax: 0.5878, Loss_oipa: 0.4575, Loss_on: 0.0534\n",
      "Epoch [25/50], Loss_minmax: 0.4766, Loss_oipa: 0.1351, Loss_on: 0.0217\n",
      "Epoch [26/50], Loss_minmax: 0.3006, Loss_oipa: 0.3406, Loss_on: 0.1199\n",
      "Epoch [27/50], Loss_minmax: 0.3276, Loss_oipa: 0.2823, Loss_on: 0.1011\n",
      "Epoch [28/50], Loss_minmax: 0.3747, Loss_oipa: 0.0154, Loss_on: 0.0025\n",
      "Epoch [29/50], Loss_minmax: 0.2712, Loss_oipa: 0.3819, Loss_on: 0.0001\n",
      "Epoch [30/50], Loss_minmax: 1.3656, Loss_oipa: 0.4949, Loss_on: 0.7178\n",
      "Epoch [31/50], Loss_minmax: 0.9495, Loss_oipa: 0.1143, Loss_on: 0.0202\n",
      "Epoch [32/50], Loss_minmax: 0.3291, Loss_oipa: 0.1011, Loss_on: 0.0001\n",
      "Epoch [33/50], Loss_minmax: 0.4128, Loss_oipa: 1.0707, Loss_on: 0.1834\n",
      "Epoch [34/50], Loss_minmax: 0.0632, Loss_oipa: 0.4358, Loss_on: 0.0003\n",
      "Epoch [35/50], Loss_minmax: 0.4785, Loss_oipa: 0.2089, Loss_on: 0.0040\n",
      "Epoch [36/50], Loss_minmax: 0.2810, Loss_oipa: 0.0001, Loss_on: 1.1219\n",
      "Epoch [37/50], Loss_minmax: 0.1667, Loss_oipa: 0.0000, Loss_on: 0.0308\n",
      "Epoch [38/50], Loss_minmax: 1.2651, Loss_oipa: 1.8490, Loss_on: 0.0014\n",
      "Epoch [39/50], Loss_minmax: 0.6944, Loss_oipa: 0.0001, Loss_on: 0.0151\n",
      "Epoch [40/50], Loss_minmax: 1.1337, Loss_oipa: 0.0000, Loss_on: 0.0023\n",
      "Epoch [41/50], Loss_minmax: 0.7135, Loss_oipa: 0.0000, Loss_on: 0.0135\n",
      "Epoch [42/50], Loss_minmax: 0.1845, Loss_oipa: 0.0530, Loss_on: 0.0000\n",
      "Epoch [43/50], Loss_minmax: 0.9658, Loss_oipa: 0.1066, Loss_on: 0.0002\n",
      "Epoch [44/50], Loss_minmax: 0.4724, Loss_oipa: 0.0058, Loss_on: 0.0000\n",
      "Epoch [45/50], Loss_minmax: 0.0154, Loss_oipa: 0.0000, Loss_on: 0.3628\n",
      "Epoch [46/50], Loss_minmax: 0.1889, Loss_oipa: 0.2279, Loss_on: 0.0032\n",
      "Epoch [47/50], Loss_minmax: 0.1272, Loss_oipa: 0.3198, Loss_on: 0.0000\n",
      "Epoch [48/50], Loss_minmax: 1.2576, Loss_oipa: 0.0045, Loss_on: 0.0000\n",
      "Epoch [49/50], Loss_minmax: 0.0483, Loss_oipa: 0.1243, Loss_on: 0.0004\n",
      "Epoch [50/50], Loss_minmax: 0.0967, Loss_oipa: 0.1757, Loss_on: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Define the model architecture\n",
    "class BinaryClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BinaryClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(90, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.sigmoid(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "# Create an instance of the model\n",
    "model_minmax = BinaryClassifier()\n",
    "model_open_inproportion_to_average = BinaryClassifier()\n",
    "model_open_normalized = BinaryClassifier()\n",
    "# Define the loss function and optimizer\n",
    "criterion_minmax = nn.BCELoss()\n",
    "criterion_oipa = nn.BCELoss()\n",
    "criterion_on = nn.BCELoss()\n",
    "optimizer_minmax = optim.Adam(model_minmax.parameters())\n",
    "optimizer_oipa = optim.Adam(model_open_inproportion_to_average.parameters())\n",
    "optimizer_on = optim.Adam(model_open_normalized.parameters())\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 50\n",
    "batch_size = 32\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    ## Run through dataloader batches\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    for i, (X_train, y_train) in enumerate(train_loader_minmax):\n",
    "    \n",
    "        # Forward pass\n",
    "        # print(X_train)\n",
    "        outputs_minmax = model_minmax(X_train)\n",
    "        y_train = y_train.unsqueeze(1)\n",
    "        loss_minmax = criterion_minmax(outputs_minmax.float(), y_train.float())\n",
    "        optimizer_minmax.zero_grad()\n",
    "        loss_minmax.backward()\n",
    "        optimizer_minmax.step()\n",
    "    for i, (X_train, y_train) in enumerate(train_loader_oipa):\n",
    "        # Forward pass\n",
    "        outputs_open_inproportion_to_average = model_open_inproportion_to_average(X_train)\n",
    "        y_train = y_train.unsqueeze(1)\n",
    "        loss_oipa = criterion_oipa(outputs_open_inproportion_to_average.float(), y_train.float())\n",
    "        optimizer_oipa.zero_grad()\n",
    "        loss_oipa.backward()\n",
    "        optimizer_oipa.step()\n",
    "    for i, (X_train, y_train) in enumerate(train_loader_on):\n",
    "        # Forward pass\n",
    "        outputs_open_normalized = model_open_normalized(X_train)\n",
    "        y_train = y_train.unsqueeze(1)\n",
    "        loss_on = criterion_on(outputs_open_normalized.float(), y_train.float())\n",
    "        # Backward pass and optimization\n",
    "        optimizer_on.zero_grad()\n",
    "        loss_on.backward()\n",
    "        optimizer_on.step()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss_minmax: {loss_minmax.item():.4f}, Loss_oipa: {loss_oipa.item():.4f}, Loss_on: {loss_on.item():.4f}')\n",
    "    \n",
    "\n",
    "# # Evaluation\n",
    "# with torch.no_grad():\n",
    "#     outputs = model(X_test)\n",
    "#     predicted = (outputs >= 0.5).float()\n",
    "#     accuracy = (predicted == y_test).float().mean()\n",
    "#     print(f'Test Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# # Make predictions\n",
    "# with torch.no_grad():\n",
    "#     outputs = model(X_new)\n",
    "#     predicted = (outputs >= 0.5).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single model eval\n",
    "# is_correct = []\n",
    "# for i, (X_train, y_train) in enumerate(validation_loader_minmax):\n",
    "    \n",
    "\n",
    "#         # print(X_train)\n",
    "#         # print(y_train)\n",
    "#         # Forward pass\n",
    "#     outputs = model(X_train)\n",
    "#     out = outputs.squeeze(1)\n",
    "#     output_bin = (out >= 0.5).int().item()\n",
    "#     correct = output_bin == int(y_train.item())\n",
    "#     is_correct.append(correct)\n",
    "# # Count ratio of trues and falses in is_correct\n",
    "# true_count = sum(is_correct)\n",
    "# false_count = len(is_correct) - true_count\n",
    "# print(f\"True count: {true_count}\")\n",
    "# print(f\"False count: {false_count}\")\n",
    "# print(f\"True ratio: {true_count / len(is_correct)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True count (Confidence): 92\n",
      "False count (Confidence): 11\n",
      "True ratio (Confidence): 0.8932038834951457\n",
      "True count (Vote): 92\n",
      "False count (Vote): 11\n",
      "True ratio (Vote): 0.8932038834951457\n",
      "True count (minmax): 77\n",
      "False count (minmax): 26\n",
      "True ratio (minmax): 0.7475728155339806\n",
      "True count (OIPA): 95\n",
      "False count (OIPA): 8\n",
      "True ratio (OIPA): 0.9223300970873787\n",
      "True count (ON): 90\n",
      "False count (ON): 13\n",
      "True ratio (ON): 0.8737864077669902\n"
     ]
    }
   ],
   "source": [
    "# Majority Voting Ensemble Eval\n",
    "is_correct_confidence = []\n",
    "is_correct_vote = []\n",
    "is_correct_minmax = []\n",
    "is_correct_oipa = []\n",
    "is_correct_on = []\n",
    "\n",
    "for i, (X_train, y_train) in enumerate(validation_loader_minmax):\n",
    "        # print(X_train)\n",
    "        # print(y_train)\n",
    "        # Forward pass\n",
    "    X_train_minmax = validation_loader_minmax.dataset.X_train[i].unsqueeze(0).float()\n",
    "    X_train_oipa = validation_loader_oipa.dataset.X_train[i].unsqueeze(0).float()\n",
    "    X_train_on = validation_loader_on.dataset.X_train[i].unsqueeze(0).float()\n",
    "    Y_train_minmax = validation_loader_minmax.dataset.Y_train[i]\n",
    "    Y_train_oipa = validation_loader_oipa.dataset.Y_train[i]\n",
    "    Y_train_on = validation_loader_on.dataset.Y_train[i]\n",
    "    # Sanity check\n",
    "    assert (Y_train_minmax.item() == Y_train_oipa.item())\n",
    "    assert (Y_train_minmax.item() == Y_train_on.item())\n",
    "\n",
    "    \n",
    "    # print(outputs_minmax)\n",
    "    outputs_minmax = model_minmax(X_train_minmax).squeeze(1)\n",
    "    outputs_oipa = model_open_inproportion_to_average(X_train_oipa).squeeze(1)\n",
    "    outputs_on = model_open_normalized(X_train_on).squeeze(1)\n",
    "\n",
    "    ## Ensemble confidence based on average of outputs\n",
    "    confidence_avg_out = (outputs_minmax + outputs_oipa + outputs_on) / 3\n",
    "    confidence_output_bin = (confidence_avg_out >= 0.5).int().item()\n",
    "\n",
    "    vote_minmax = (outputs_minmax >= 0.5).int().item()\n",
    "    is_correct_minmax.append(vote_minmax == int(y_train.item()))\n",
    "    vote_oipa = (outputs_oipa >= 0.5).int().item()\n",
    "    is_correct_oipa.append(vote_oipa == int(y_train.item()))\n",
    "    vote_on = (outputs_on >= 0.5).int().item()\n",
    "    is_correct_on.append(vote_on == int(y_train.item()))\n",
    "    ## Ensemble voting based on majority voting\n",
    "    vote = (vote_minmax + vote_oipa + vote_on) >= 2\n",
    "    correct_confidence = confidence_output_bin == int(y_train.item())\n",
    "    correct_vote = vote == int(y_train.item())\n",
    "    is_correct_confidence.append(correct_confidence)\n",
    "    is_correct_vote.append(correct_vote)\n",
    "\n",
    "    # out = outputs.squeeze(1)\n",
    "    # output_bin = (out >= 0.5).int().item()\n",
    "    # correct = output_bin == int(y_train.item())\n",
    "    # is_correct.append(correct)\n",
    "# Count ratio of trues and falses in is_correct\n",
    "true_count_confidence = sum(is_correct_confidence)\n",
    "false_count_confidence = len(is_correct_confidence) - true_count_confidence\n",
    "print(f\"True count (Confidence): {true_count_confidence}\")\n",
    "print(f\"False count (Confidence): {false_count_confidence}\")\n",
    "print(f\"True ratio (Confidence): {true_count_confidence / len(is_correct_confidence)}\")\n",
    "true_count_vote = sum(is_correct_vote)\n",
    "false_count_vote = len(is_correct_vote) - true_count_vote\n",
    "print(f\"True count (Vote): {true_count_vote}\")\n",
    "print(f\"False count (Vote): {false_count_vote}\")\n",
    "print(f\"True ratio (Vote): {true_count_vote / len(is_correct_vote)}\")\n",
    "true_count_minmax = sum(is_correct_minmax)\n",
    "false_count_minmax = len(is_correct_minmax) - true_count_minmax\n",
    "print(f\"True count (minmax): {true_count_minmax}\")\n",
    "print(f\"False count (minmax): {false_count_minmax}\")\n",
    "print(f\"True ratio (minmax): {true_count_minmax / len(is_correct_minmax)}\")\n",
    "true_count_oipa = sum(is_correct_oipa)\n",
    "false_count_oipa = len(is_correct_oipa) - true_count_oipa\n",
    "print(f\"True count (OIPA): {true_count_oipa}\")\n",
    "print(f\"False count (OIPA): {false_count_oipa}\")\n",
    "print(f\"True ratio (OIPA): {true_count_oipa / len(is_correct_oipa)}\")\n",
    "true_count_on = sum(is_correct_on)\n",
    "false_count_on = len(is_correct_on) - true_count_on\n",
    "print(f\"True count (ON): {true_count_on}\")\n",
    "print(f\"False count (ON): {false_count_on}\")\n",
    "print(f\"True ratio (ON): {true_count_on / len(is_correct_on)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Count ratio of trues and falses in is_correct\n",
    "# true_count = sum(is_correct)\n",
    "# false_count = len(is_correct) - true_count\n",
    "# print(f\"True count: {true_count}\")\n",
    "# print(f\"False count: {false_count}\")\n",
    "# print(f\"True ratio: {true_count / len(is_correct)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save each model\n",
    "directory = \"classifier_models/\"\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "torch.save(model_minmax.state_dict(), directory +\"minmax_model.pth\")\n",
    "torch.save(model_open_inproportion_to_average.state_dict(), directory +\"open_inproportion_to_average_model.pth\")\n",
    "torch.save(model_open_normalized.state_dict(), directory +\"open_normalized_model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'classifier_models/open_inproportion_to_average_model_2.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m model_open_normalized \u001b[38;5;241m=\u001b[39m BinaryClassifier()\n\u001b[0;32m      5\u001b[0m model_minmax\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(directory \u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mminmax_model.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m----> 6\u001b[0m model_open_inproportion_to_average\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mopen_inproportion_to_average_model_2.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      7\u001b[0m model_open_normalized\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(directory \u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopen_normalized_model_2.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\alexs\\anaconda3\\envs\\cs828\\Lib\\site-packages\\torch\\serialization.py:998\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m    995\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    996\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 998\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[0;32m    999\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m   1000\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m   1001\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m   1002\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m   1003\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[1;32mc:\\Users\\alexs\\anaconda3\\envs\\cs828\\Lib\\site-packages\\torch\\serialization.py:445\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    443\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    444\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 445\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    446\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    447\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[1;32mc:\\Users\\alexs\\anaconda3\\envs\\cs828\\Lib\\site-packages\\torch\\serialization.py:426\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    425\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[1;32m--> 426\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'classifier_models/open_inproportion_to_average_model_2.pth'"
     ]
    }
   ],
   "source": [
    "# Load in saved models\n",
    "model_minmax = BinaryClassifier()\n",
    "model_open_inproportion_to_average = BinaryClassifier()\n",
    "model_open_normalized = BinaryClassifier()\n",
    "model_minmax.load_state_dict(torch.load(directory +\"minmax_model.pth\"))\n",
    "model_open_inproportion_to_average.load_state_dict(torch.load(directory +\"open_inproportion_to_average_model_2.pth\"))\n",
    "model_open_normalized.load_state_dict(torch.load(directory +\"open_normalized_model_2.pth\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs828",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
